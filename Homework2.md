# HW-2

**Name:** Ganguly, Shreyashi

**NetID:** sgz9928

GitHub link - https://github.com/MSIA/sgz9928_msia_text_analytics_2020.git
Branch - homework2

## Problem 1

Word2vec implementation of the python gensim library was utilized to obtain the word embeddings.

The text corpus chosen was a collection of news articles pertaining to politicals and guns (20_newsgroups/talk.politics.guns) <br>
Number of files parsed = 1000<br>
Total size of files parsed = 2.6MB

The data was preprocessed as follows,
- converted to lower case
- numbers removed
- punctuations removed
- white leading and trailing spaces removed
- tokenized
- stop words removed

The relevant script for this step - <br>
Some example processed files can be found - 

The following parameters were experimented with,
- CBOW/skip-gram
- min_count = 10, 50
- size = 50, 100, 150
- window = 2, 3, 5

In order to evaluate the vector embeddings generated by each of the models, the closest 10 words of the following words were examined,
- gun
- america
- peace
- safety
- violence

Upon inspection of the closest words, the skip gram model with 100 nodes, window size of 2 and mincount of 10 gave the most semantically intuitive results.

Top 10 most similar words to "gun":<br>
 ['handgun', 'express', 'american', 'laws', 'card', 'proper', 'mad', 'viability', 'like', 'crime']

Top 10 most similar words to "america": <br>
['examples', 'legislation', 'terminal', 'prevent', 'officials', 'students', 'mississippi', 'militias', 'title', 'policy']

Top 10 most similar words to "peace": <br>
 ['superior', 'permitted', 'mostly', 'define', 'tired', 'requires', 'effort', 'narrow', 'particular', 'island']

Top 10 most similar words to "safety": <br>
 ['threat', 'uk', 'ordinary', 'lives', 'ok', 'impression', 'accidents', 'banning', 'risk', 'class']

Top 10 most similar words to "violence": <br>
 ['machine', 'home', 'ownership', 'buyback', 'effect', 'nazi', 'main', 'times', 'crimes', 'sounds']

 Relevant python scripts for training and evaluating word2vec models - <br>
 Detailed results of each of the fitted model can be found - here


## Problem 2

Comparison of word2vec and Bert:

|  Aspect   |  word2vec   |  Bert  |
| --- | --- | --- |
|  Learning Approach   |  skipgram: given a word predict its neighbours or context<br>cbow: given the context predict the word  |  Masked LM: Some percentage of input tokens are masked at random, which the model then predicts<br> Next Sentence Prediction: Given a sentence, predict the next  |
|  Embeddings   |  Context independent. Model serving is essentially a dictionary lookup task  |  Embeddings are context dependent, i.e., dependent on the position of the word  |
|  Practical Application   |  word2vec being context independent, embeddings trained on a large enough corpus can be directly used in down stream NLP applications, without retaining the original model  |  The Bert model needs to be retained to generate embeddings for words depending on their context  |
|  Corpus Size  |  Large amount of training data crucial. Originally trained on a dataset with 33 billion words  |  Document level corpus necessary. Pre trained on BooksCorpus (800M words) and English Wikipedia (2,500M words)  |
|  Citations   |  Published in 2013<br>Number of citations 22,851  |  Published in 2018<br>Number of citations 10,265  |




